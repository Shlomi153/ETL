{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed26fb-1bd5-44be-be56-58963afe846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: focus is on building the database, schemas, tables, configuration tables and columns needed in order to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03e7f5-534e-4878-87dd-b2a6f68d3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import sqlalchemy as alch\n",
    "import urllib as lib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4543c-e2ac-4f42-b245-2e1020a1be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define OS system variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c7ed8-7fa5-43ad-b4da-4720650c29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('ReplicaSource_Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d168bdf-5986-4d16-8001-fad184da5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define connections for initialDB - SQLAlchemy\n",
    "targetServer = 'localhost'\n",
    "mssqlDriverSQLAlchemy = 'SQL SERVER NATIVE CLIENT 11.0'\n",
    "\n",
    "quotedInitalDB = lib.parse.quote_plus('DRIVER='+mssqlDriverSQLAlchemy+';SERVER='+targetServer+';Trusted_Connection=yes')\n",
    "engineInitalDB = alch.create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quotedInitalDB), future=True, connect_args = {'autocommit':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd1f0a-56b5-4ae0-ac4a-d25e7ab431d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engineInitalDB.connect() as initialDbCheck:\n",
    "    createDbIfNotExists = \"\"\"\n",
    "        IF NOT EXISTS(SELECT * FROM sys.databases WHERE name = 'ReplicaSource')\n",
    "    \t    CREATE DATABASE ReplicaSource\n",
    "    \"\"\"\n",
    "    createDbCmd = alch.text(createDbIfNotExists)\n",
    "\n",
    "    initialDbCheck.execute(createDbCmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b1719-1e13-42bb-adf2-aa4a33f7e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define connections for initialDBSchemas - SQLAlchemy\n",
    "targetDatabase = 'ReplicaSource'\n",
    "\n",
    "quotedInitalSchema = lib.parse.quote_plus('DRIVER='+mssqlDriverSQLAlchemy+';SERVER='+targetServer+';DATABASE='+targetDatabase+';Trusted_Connection=yes')\n",
    "engineInitialSchema = alch.create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quotedInitalSchema), future=True, connect_args = {'autocommit':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2e389-b65c-4f56-b328-9a7294e7c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engineInitialSchema.connect() as initialSchemaCheck:\n",
    "    createSchemaIfNotExists = \"\"\"\n",
    "        IF NOT EXISTS(SELECT * FROM sys.schemas WHERE name = 'Import')\n",
    "            EXEC('CREATE SCHEMA Import AUTHORIZATION [dbo]');\n",
    "\n",
    "\t\tIF NOT EXISTS(SELECT * FROM sys.schemas WHERE name = 'Helper')\n",
    "\t\t\tEXEC('CREATE SCHEMA Helper AUTHORIZATION [dbo]')\n",
    "    \"\"\"\n",
    "\n",
    "    createSchemaCmd = alch.text(createSchemaIfNotExists)\n",
    "    initialSchemaCheck.execute(createSchemaCmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d00a04-4f44-4747-8fd6-3928b463d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data from Source1 - Spark\n",
    "source1Host = ''\n",
    "source1DB = ''\n",
    "source1Port = ''\n",
    "\n",
    "source1Properties = {\n",
    "    \"user\": \"\",\n",
    "    \"password\": \"\",\n",
    "    \"mysqlDriver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "source1URL = f'jdbc:mysql://{source1Host}:{source1Port}/{source1DB}?tinyInt1isBit=false&zeroDateTimeBehavior=convertToNull'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ce090-06d8-418f-80df-84142c758afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "source1InfoSchema = \"\"\"(\n",
    "\n",
    ")AS t\n",
    ")AS source1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8316d-c23f-4b1d-99bc-ce2349558fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source1InfoSchema = spark.read.jdbc(url = source1URL, table = source1InfoSchema, properties = source1Properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4e867-0f62-4015-b864-aa3833829dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source1InfoSchema.createOrReplaceTempView('Source1InfoSchemaView')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0062a27-fcc0-4a09-b20a-06be03c1dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "source1InfoSchemaView = spark.sql(\"\"\"SELECT DISTINCT * FROM Source1InfoSchemaView\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cbe64-d590-42a9-b3a8-34c23c510fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "source1InfoSchemaView.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393095a3-175f-4f15-a591-37a9f4e6e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case you have multiple data sources\n",
    "#mergeAllInfoSchemas = source1InfoSchemaView.union(source2InfoSchemaView).union(source3InfoSchemaView).union(source4InfoSchemaView)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0989dc-655d-4d09-90de-e0c620b9f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeAllInfoSchemas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e506cb-adea-49ad-bcbc-75faea788bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete columns that were not cleaned up in Information_Schema\n",
    "cleanupMeta = f\"\"\"\n",
    "    DELETE FROM Import.SourceMetaData\n",
    "\tWHERE CONCAT(Source,'.', TABLE_SCHEMA,'.', TABLE_NAME,'.', COLUMN_NAME,'.', ColumnDefinition) IN\n",
    "\t(\n",
    "\t\tSELECT DISTINCT CONCAT(Source,'.', TABLE_SCHEMA,'.', TABLE_NAME,'.', COLUMN_NAME,'.', ColumnDefinition)\n",
    "\t\tFROM\n",
    "\t\t(\n",
    "\t\t\tSELECT Source, TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, ColumnDefinition, DENSE_RANK() OVER(PARTITION BY Source, TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME ORDER BY ColumnDefinition) AS RankColDefinition\n",
    "\t\t\tFROM Import.SourceMetaData AS  d\n",
    "\t\t)AS t\n",
    "\t\tWHERE RankColDefinition > 1\n",
    "\t)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b940e48-fee8-41d5-8764-2832204ae070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define connection with SQLAlchemy for Config file\n",
    "mssqlDriverSQLAlchemy = 'SQL SERVER NATIVE CLIENT 11.0'\n",
    "targetSchema = 'Import'\n",
    "configTable = targetSchema + '.' + 'TablesToLoad'\n",
    "metaData = targetSchema + '.' + 'SourceMetaData'\n",
    "\n",
    "quotedConfig = lib.parse.quote_plus('DRIVER='+mssqlDriverSQLAlchemy+';SERVER='+targetServer+';DATABASE='+targetDatabase+';Trusted_Connection=yes')\n",
    "engineConfig = alch.create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quotedConfig), future=True, connect_args = {'autocommit':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0107cf1-0522-473c-837c-bcf672ec829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define connections for Import Table - Spark\n",
    "mssqlDriverSpark = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "targetURL = f\"jdbc:sqlserver://localhost:1433;databaseName={targetDatabase};integratedSecurity=true;encrypt=false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6bb01f-4d08-424e-a04c-468f8cc8f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write import table to Replica DB, this holds the table schemas and names - Spark\n",
    "mergeAllInfoSchemas.write \\\n",
    "                .format('jdbc') \\\n",
    "                .mode('overwrite') \\\n",
    "                .option('truncate', 'true') \\\n",
    "                .option('url', targetURL) \\\n",
    "                .option('driver', mssqlDriverSpark) \\\n",
    "                .option('dbtable', metaData) \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a553cd-d266-471a-b5d9-9212a60818a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the cleanup, cleanup for metaData that has duplicates in SourceDB's\n",
    "with engineConfig.begin() as transactionsCleanup:\n",
    "    cleanupMeta = f\"\"\"{cleanupMeta}\"\"\"\n",
    "    cleanupMetaCmd = alch.text(cleanupMeta)\n",
    "    transactionsCleanup.execute(cleanupMetaCmd)\n",
    "transactionsCleanup.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fb97c-14c5-40fa-b7e6-fd1ba0599b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeAllInfoSchemas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b120fdc0-7538-4df2-8da7-2cfb6213d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the Config table if it doesn't already exist\n",
    "with engineConfig.begin() as config:\n",
    "    createTableIfNotExists = f\"\"\"\n",
    "    IF NOT EXISTS(\n",
    "      SELECT 1 FROM sys.tables AS t\n",
    "      INNER JOIN sys.schemas AS s\n",
    "      ON(t.schema_id = s.schema_id)\n",
    "      WHERE s.name = 'Import' AND t.name = 'TablesToLoad'\n",
    "    )\n",
    "    BEGIN\n",
    "    \tCREATE TABLE {configTable}\n",
    "    \t(\n",
    "    \t\tSource VARCHAR(50),\n",
    "    \t\tDbName VARCHAR(50),\n",
    "    \t\tTableName NVARCHAR(200),\n",
    "    \t\tToLoad CHAR(1),\n",
    "    \t\tHelper nvarchar(max)\n",
    "    \t)\n",
    "    END\n",
    "    \"\"\"\n",
    "    createTableCmd = alch.text(createTableIfNotExists)\n",
    "    config.execute(createTableCmd)\n",
    "    config.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916dff15-9940-4f1e-ba9a-345fab7e8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge tables that do not exist in the table\n",
    "with engineConfig.begin() as transaction:\n",
    "    mergeIntoStatement = f\"\"\"\n",
    "    MERGE {configTable} AS Target\n",
    "    USING (SELECT DISTINCT Source, TABLE_SCHEMA, TABLE_NAME, CONCAT(TABLE_SCHEMA,'.', TABLE_NAME) AS Helper \n",
    "    FROM {metaData}) AS Source\n",
    "    ON(CONCAT(Target.Source, '.', Target.DbName,'.', Target.TableName) = CONCAT(Source.Source, Source.Helper))\n",
    "    WHEN NOT MATCHED THEN \n",
    "    INSERT\n",
    "    (Source, DbName, TableName, Helper)\n",
    "    VALUES\n",
    "    (Source.Source, Source.TABLE_SCHEMA, Source.TABLE_NAME, Source.Helper);\n",
    "    \"\"\"\n",
    "    mergeCmdQuery = alch.text(mergeIntoStatement)\n",
    "    transaction.execute(mergeCmdQuery)\n",
    "\n",
    "    transaction.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde5cf7-57e4-413d-957e-e476bae86403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read from config file to see which tables should be loaded\n",
    "schemasStructToCreate = f\"\"\"(\n",
    "    SELECT DISTINCT DbName AS SchemaName\n",
    "    FROM {configTable} AS t\n",
    "    LEFT JOIN sys.schemas AS schem\n",
    "    ON(t.DbName = schem.name)\n",
    "    WHERE t.ToLoad = 1 AND schem.name IS NULL\n",
    "    )\"\"\"\n",
    "\n",
    "tablesStructToCreate = f\"\"\"(\n",
    "    SELECT DISTINCT SchemaName, TableName\n",
    "    FROM \n",
    "    (\n",
    "    SELECT conf.Source, conf.DbName AS SchemaName, conf.TableName, meta.COLUMN_NAME AS ColumnName, ColumnDefinition AS ColumnType\n",
    "    , REPLACE(REPLACE(conf.Helper, '[', ''), ']', '') AS Helper\n",
    "    FROM {configTable} AS conf\n",
    "    INNER JOIN {metaData} AS meta\n",
    "    ON(conf.Source = meta.Source AND conf.DbName = meta.TABLE_SCHEMA AND conf.TableName = meta.TABLE_NAME)\n",
    "    WHERE conf.ToLoad = 1 \n",
    "    )AS t\n",
    "    LEFT JOIN INFORMATION_SCHEMA.TABLES AS infoTab\n",
    "    ON(Helper = CONCAT(infoTab.TABLE_SCHEMA, '.', infoTab.TABLE_NAME))\n",
    "    --WHERE TABLE_CATALOG IS NULL\n",
    "  )\"\"\"\n",
    "\n",
    "columnsStructToCreate = f\"\"\"(\n",
    "\tSELECT DISTINCT SchemaName, TableName, ColumnName, ColumnType\n",
    "    FROM \n",
    "    (\n",
    "    SELECT conf.Source, conf.DbName AS SchemaName, conf.TableName, meta.COLUMN_NAME AS ColumnName, ColumnDefinition AS ColumnType\n",
    "    , REPLACE(REPLACE(conf.Helper, '[', ''), ']', '') AS Helper, meta.COLUMN_NAME\n",
    "    FROM Import.TablesToLoad AS conf\n",
    "    INNER JOIN Import.SourceMetaData AS meta\n",
    "    ON(conf.Source = meta.Source AND conf.DbName = meta.TABLE_SCHEMA AND conf.TableName = meta.TABLE_NAME)\n",
    "    WHERE conf.ToLoad = 1 \n",
    "    )AS t\n",
    "    LEFT JOIN INFORMATION_SCHEMA.COLUMNS AS infoCol\n",
    "    ON(Helper = CONCAT(infoCol.TABLE_SCHEMA, '.', infoCol.TABLE_NAME)  AND ColumnName = infoCol.COLUMN_NAME)\n",
    "    --WHERE infoCol.TABLE_CATALOG IS NULL\n",
    ")\"\"\"\n",
    "\n",
    "dropAllTablesExceptImport = f\"\"\"(\n",
    "\tSELECT DISTINCT TABLE_SCHEMA AS SchemaName, Table_Name AS TableName\n",
    "\tFROM INFORMATION_SCHEMA.TABLES AS tab\n",
    "\tWHERE TABLE_SCHEMA != 'Import'\n",
    " )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bcace-21fd-4ed2-b49d-7b5a5e8f2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define connections for RequiredDBSchemas\n",
    "quotedRequiredStructs = lib.parse.quote_plus('DRIVER='+mssqlDriverSQLAlchemy+';SERVER='+targetServer+';DATABASE='+targetDatabase+';Trusted_Connection=yes')\n",
    "engineRequiredStructs = alch.create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quotedRequiredStructs), future=True, connect_args = {'autocommit':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a496f-a268-426e-b4a2-2f81b6e9f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Schemas if they are required and don't currently exist\n",
    "with engineRequiredStructs.begin() as transactionsList:\n",
    "    listOfSchemas = alch.text(f\"\"\"{schemasStructToCreate}\"\"\")\n",
    "    listOfSchemasCmd = transactionsList.execute(listOfSchemas).fetchall()\n",
    "    transactionsList.commit()\n",
    "\n",
    "with engineRequiredStructs.begin() as transactionsCreate:\n",
    "    for row in listOfSchemasCmd:\n",
    "        createSchema = f\"\"\"CREATE SCHEMA {row[0]}\"\"\"\n",
    "        createSchemaCmd = alch.text(createSchema)\n",
    "        transactionsCreate.execute(createSchemaCmd)\n",
    "    transactionsCreate.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbff791-c9bb-4850-9d16-6fdb6f6ca32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Tables if they are required\n",
    "with engineRequiredStructs.begin() as transactionsList:\n",
    "    listOfTables = alch.text(f\"\"\"{tablesStructToCreate}\"\"\")\n",
    "    listOfTablesCmd = transactionsList.execute(listOfTables).fetchall()\n",
    "    transactionsList.commit()\n",
    "\n",
    "#Drop all tables, save for Import Schema and then create them again\n",
    "with engineRequiredStructs.begin() as transactionsDropList:      \n",
    "    listOfTablesToDrop = alch.text(f\"\"\"{dropAllTablesExceptImport}\"\"\")\n",
    "    listOfTablesToDropCmd = transactionsDropList.execute(listOfTablesToDrop).fetchall()\n",
    "    transactionsDropList.commit()\n",
    "\n",
    "with engineRequiredStructs.begin() as transactionsDropTables:\n",
    "    for row in listOfTablesToDropCmd:\n",
    "        dropTable = f\"\"\"DROP TABLE {row.SchemaName}.[{row.TableName}]\"\"\"\n",
    "        dropTableCmd = alch.text(dropTable)\n",
    "        transactionsDropTables.execute(dropTableCmd)\n",
    "    transactionsDropTables.commit()\n",
    "\n",
    "with engineRequiredStructs.begin() as transactionsCreate: \n",
    "    for row in listOfTablesCmd:\n",
    "        createTable = f\"\"\"CREATE TABLE {row.SchemaName}.{row.TableName}\"\"\"\n",
    "        createTableWithDummy = createTable + '(Dummy int)'\n",
    "        \n",
    "        createTableCmd = alch.text(createTableWithDummy)\n",
    "        transactionsCreate.execute(createTableCmd)\n",
    "    transactionsCreate.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10df62-382e-4585-aa98-8f8e05c70c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix data types of specific tables like events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d556ebe-9d63-4205-a7bb-f9086e1862f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Columns if they are required and don't currently exist\n",
    "with engineRequiredStructs.begin() as transactionsList:\n",
    "    listOfColumns = alch.text(f\"\"\"{columnsStructToCreate}\"\"\")\n",
    "    listOfColumnsCmd = transactionsList.execute(listOfColumns).fetchall()\n",
    "\n",
    "    transactionsList.commit()\n",
    "\n",
    "#Add columns to the tables\n",
    "with engineRequiredStructs.begin() as transactionsAddColumns:\n",
    "    for row in listOfColumnsCmd:\n",
    "        addColumns = f\"\"\"ALTER TABLE {row.SchemaName}.{row.TableName} ADD {row.ColumnName} {row.ColumnType}\"\"\"\n",
    "        addColumnsCmd = alch.text(addColumns)\n",
    "        transactionsAddColumns.execute(addColumnsCmd)\n",
    "        \n",
    "    transactionsAddColumns.commit()\n",
    "\n",
    "#Remove dummy columns\n",
    "with engineRequiredStructs.begin() as transactionsRemoveDummy:\n",
    "    for row in listOfTablesCmd:\n",
    "        deleteDummyCol = f\"\"\"ALTER TABLE {row.SchemaName}.{row.TableName} DROP COLUMN Dummy\"\"\"\n",
    "        deleteDummyColCmd = alch.text(deleteDummyCol)\n",
    "        transactionsRemoveDummy.execute(deleteDummyColCmd)\n",
    "\n",
    "transactionsRemoveDummy.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93fa54-49ed-46d9-825a-a86d2cc1b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: now that the infrastructure has been built, we can safely load the data from each SourceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d7741-02e7-4453-b36d-070eb7fafe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all relevant tables to load in general\n",
    "tablesToLoadQuery = f\"\"\"(\n",
    "SELECT DISTINCT Source, DbName, REPLACE(REPLACE(TableName, '[', ''), ']', '') AS TableName, REPLACE(REPLACE(Helper, '[', ''), ']', '') AS SourceHelper, Helper AS TargetHelper\n",
    "FROM {configTable} AS conf\n",
    "WHERE ToLoad = 1\n",
    ")AS TablesToLoad\n",
    "\"\"\"\n",
    "df_tablesToLoad = spark.read \\\n",
    "                .format('jdbc') \\\n",
    "                .option('url', targetURL) \\\n",
    "                .option('driver', mssqlDriverSpark) \\\n",
    "                .option('dbtable', tablesToLoadQuery) \\\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff45ce2-df39-4323-9982-5de7f2d5e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tablesToLoad.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ec6e2-2441-46d3-ba35-830d1521f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate to different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd63b46-c89d-4adb-873b-d716e242265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather data from source1 Databases\n",
    "df_source1 = df_tablesToLoad.filter(\"Source == '' AND DbName NOT IN('', '', '')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908bd7b-2988-43e2-903d-ee38b0691341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source1.createOrReplaceTempView('Source1View')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f6193-7bc2-41f6-a22a-9bd63cd6d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "source1View = spark.sql(\"\"\"SELECT DISTINCT SourceHelper, TargetHelper FROM Source1View\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8670284-fde0-4078-84c0-e5d9c9cc1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "source1View.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e4b19-5744-42e3-a26b-a40d61254302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather the relevant Source and Target tables in a list of tuples for source1\n",
    "source1TablesList = source1View.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f4bcd-129f-4daf-a831-c34c24a07f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in source1TablesList:\n",
    "    #print(row.SourceHelper, row.TargetHelper)\n",
    "    readTableQuery = f\"\"\"(\n",
    "    SELECT DISTINCT * FROM {row.SourceHelper} \n",
    "    )AS ReadTable\n",
    "    \"\"\"\n",
    "    df_readTable = spark.read.jdbc(url = sourceSource1URL, table = readTableQuery, properties = source1Properties)\n",
    "    df_readTable.write \\\n",
    "                .format('jdbc') \\\n",
    "                .mode('overwrite') \\\n",
    "                .option('truncate', 'true') \\\n",
    "                .option('url', targetURL) \\\n",
    "                .option('driver', mssqlDriverSpark) \\\n",
    "                .option('dbtable', row.TargetHelper) \\\n",
    "                .save()\n",
    "    #df_readTable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2ec13-ea1b-47db-9a4b-22b9b80d161e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf88024-36ea-4a6a-847f-2cf363ba4446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d641b-a12c-489e-831a-8089f7a21e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca1c6d-1d0a-4fce-b623-5cd965c036cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b558a1-731c-447f-b545-0c9518871860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
